# benchmarking

# ğŸ” Gemma-Bench: A Benchmarking Suite for Evaluating Gemma and Other Open LLMs

Welcome to **Gemma-Bench** â€” a reproducible, extensible, and automated benchmarking framework for evaluating **Gemma models** on academic and custom datasets, and comparing them against other popular open-source models like **LLaMA 2**, **Mistral**, and **Falcon**.

> ğŸ§ª This is a proof-of-concept for a full-scale GSoC 2025 project proposal. Contributions, feedback, and ideas are welcome!

---

## ğŸš€ Goals

- Benchmark **Gemma model variants** (2B, 7B, etc.) on standard academic datasets (MMLU, GSM8K, ARC, HellaSwag).
- Compare performance against other open LLMs (LLaMA 2, Mistral).
- Automate the evaluation process with modular scripts.
- Generate **visual reports** (charts, tables) and support **live leaderboard-style views**.
- Ensure **reproducibility** through Docker and simple CLI interfaces.
- Enable custom benchmarks (e.g., enterprise QA agent tasks inspired by real-world use cases).

---

## ğŸ› ï¸ Features (Planned)

âœ… Evaluate models on multiple-choice and generative tasks  
âœ… Support for Gemma and Hugging Face-compatible LLMs  
âœ… Clean output format (JSON/CSV)  
âœ… Visualization via Matplotlib/Plotly  
âœ… Modular dataset and model registration  
ğŸš§ Leaderboard generation (coming soon)  
ğŸš§ Web UI / dashboard (optional)  


---

## ğŸ“Š Sample Output

| Model      | MMLU Accuracy | GSM8K EM | Inference Time |
|------------|----------------|----------|----------------|
| Gemma 2B   | 34.2%          | 42.5%    | 1.2s           |
| LLaMA 2 7B | 45.3%          | 54.1%    | 1.7s           |
| Mistral 7B | 48.1%          | 56.3%    | 1.5s           |

ğŸ“ˆ More visual charts to come!

---

## ğŸ§ª Quick Start

```bash
git clone https://github.com/shikharxyz/gemma-bench.git
cd gemma-bench
pip install -r requirements.txt

# Run MMLU benchmark for Gemma 2B
python run_mmlu.py --model gemma-2b

# Datasets
MMLU

GSM8K

ARC

HellaSwag

Custom QA Evaluation (WIP)

ğŸ” Reproducibility
This project will support:

Docker-based reproducible environments

Config-driven evaluation pipelines

Open data and clear evaluation logs

ğŸŒ± GSoC 2025 Context
This repo is part of my Google Summer of Code 2025 proposal titled:

"Benchmarking Gemma: A Reproducible and Scalable Suite for Evaluating Open Models"

Key goals:

Build a sustainable tool for the open-source community.

Contribute back to the Gemma ecosystem.

Help LLM researchers and developers compare models with confidence.

ğŸ™‹â€â™‚ï¸ About Me
I'm Shikhar, an MS student in IT & Analytics with 6+ years of industry experience as a Software Developer at Walmart. I've built LLM-based QA agents for enterprise use, and I'm passionate about bridging research and engineering in the LLM space.

Feel free to connect or collaborate!

ğŸ§  License
MIT License. All datasets are subject to their respective licenses.
